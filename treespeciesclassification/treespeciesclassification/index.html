
<!DOCTYPE html>

<html class="no-js" lang="de">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../../1_Laserscanning_lidR/1_Laserscanning_lidR/" rel="prev"/>
<link href="../../BFAST/BFAST/" rel="next"/>
<link href="../../assets/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.0.15" name="generator"/>
<title>Tree species classification - Fernerkundung und Geoinformatik</title>
<link href="../../assets/stylesheets/main.113286f1.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.a0c5b2b5.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="light-green" data-md-color-primary="white" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#classification-of-tree-species-using-hyperspectral-data">
          Zum Inhalt
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Kopfzeile" class="md-header__inner md-grid">
<a aria-label="Fernerkundung und Geoinformatik" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Fernerkundung und Geoinformatik">
<img alt="logo" src="../../assets/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Fernerkundung und Geoinformatik
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Tree species classification
            
          </span>
</div>
</div>
</div>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Suche" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Suche" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Suche" class="md-search__options">
<button aria-label="Zurücksetzen" class="md-search__icon md-icon" tabindex="-1" title="Zurücksetzen" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Suche wird initialisiert
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/Remote-Sensing-at-FU-Berlin/Remote-Sensing-at-FU-Berlin.github.io/" title="Zum Repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    Remote-Sensing-at-FU-Berlin.github.io
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="Hauptnavigation" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../..">
      Welcome
    </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link md-tabs__link--active" href="../../qgis/01-GUI/">
        Fabian
      </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Fernerkundung und Geoinformatik" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Fernerkundung und Geoinformatik">
<img alt="logo" src="../../assets/logo.png"/>
</a>
    Fernerkundung und Geoinformatik
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/Remote-Sensing-at-FU-Berlin/Remote-Sensing-at-FU-Berlin.github.io/" title="Zum Repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</div>
<div class="md-source__repository">
    Remote-Sensing-at-FU-Berlin.github.io
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Welcome
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Fabian
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          Fabian
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
          QGIS
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_1">
<span class="md-nav__icon md-icon"></span>
          QGIS
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/01-GUI/">
        Getting to know the graphical user interface
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/02-vector-data/">
        Loading and visualizing Shapefiles / Vector data
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/03-raster-data/">
        Loading and visualizing raster data
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/04-general-tools/">
        Useful general tools
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/05-digitizing/">
        Basic vector processing
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/06-selections/">
        Spatial and semantic selection
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/07-elevation/">
        Working with elevation raster datasets
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/08-map-making/">
        Making a map
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../qgis/09-spatial-interpolation/">
        Spatial interpolation
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
          R
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_2">
<span class="md-nav__icon md-icon"></span>
          R
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../1_Laserscanning_lidR/1_Laserscanning_lidR/">
        lidR Lasercanning
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Tree species classification
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Tree species classification
      </a>
<nav aria-label="Inhaltsverzeichnis" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Inhaltsverzeichnis
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#classification-of-tree-species-using-hyperspectral-data">
    Classification of tree species using hyperspectral data
  </a>
<nav aria-label="Classification of tree species using hyperspectral data" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#overview">
    Overview
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#datasets-used-in-this-tutorial">
    Datasets used in this Tutorial
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#step-1-loading-an-visualizing-the-hyperspectral-hymap-image-and-the-reference-dataset">
    Step 1: Loading an visualizing the hyperspectral HyMap image and the reference dataset
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#step-2-tree-species-classification-using-the-original-hymap-bands">
    Step 2: Tree species classification using the original HyMap bands
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#step-3-tree-species-classification-using-pca-bands">
    Step 3: Tree species classification using PCA bands
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#step-4-tree-species-classification-using-a-combination-of-feature-selection-vsurf-and-feature-extraction-pca">
    Step 4: Tree species classification using a combination of feature selection (VSURF) and feature extraction (PCA)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#exercise">
    Exercise
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../BFAST/BFAST/">
        Analysing time series with BFAST and NPPHEN
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../landscape_metrics/landscape_metrics/">
        Calculation of Landscape Metrics from a Land-cover raster map in R
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../spatial_autocorrelation/spatial_autocorrelation/">
        Calculating semivariograms and Autocorrelograms
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
          GEE
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_3">
<span class="md-nav__icon md-icon"></span>
          GEE
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../MSRS_1_GEE_basics/MSRS_1_GEE_basics/">
        Preparing time series data stacks with Google Earth Engine
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../MSRS_1_GEE_basics2/MSRS_1_GEE_basics2/">
        Preparing annual time series data stacks with Google Earth Engine and conducting time series analysis in R
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
          SNAP
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_4">
<span class="md-nav__icon md-icon"></span>
          SNAP
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../planetary-sciences/01-download-ls-and-s2/">
        Download of Landsat and Sentinel satellite images
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/Remote-Sensing-at-FU-Berlin/Remote-Sensing-at-FU-Berlin.github.io/tree/main/docs/treespeciesclassification/treespeciesclassification.md" title="Seite editieren">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"></path></svg>
</a>
<h1>Tree species classification</h1>
<h2 id="classification-of-tree-species-using-hyperspectral-data">Classification of tree species using hyperspectral data<a class="headerlink" href="#classification-of-tree-species-using-hyperspectral-data" title="Link zu dieser Sektion">¶</a></h2>
<h3 id="overview">Overview<a class="headerlink" href="#overview" title="Link zu dieser Sektion">¶</a></h3>
<p>In this lecture you will learn how to use hyperspectral data in combination with a support vector machines (SVM) algorithm to create a tree species map from a hyperspectral HyMap image using a supervised classification. The learned steps will include:</p>
<ul>
<li>Loading a hyperspectral  image</li>
<li>Visualizing a hyperspectral image and vegetation spectra extracted from pixels</li>
<li>Conduct a supervised SVM classification using the original HyMap bands</li>
<li>Applying a PCA transformation to the Hyperspectral image (feature extraction) </li>
<li>Conduct a supervised SVM classification using the PCA components as input</li>
<li>Apply an additional feature selection to the PCA components and conduct a supervised SVM classification using an automatically selected subset of the PCA components</li>
</ul>
<p>The datasets applied in this tutorial are available here:</p>
<p><a href="https://drive.google.com/file/d/1OkOcvNTDGR5PcBhress0pbwMP5wPqVui/view?usp=sharing">https://drive.google.com/file/d/1OkOcvNTDGR5PcBhress0pbwMP5wPqVui/view?usp=sharing</a></p>
<h3 id="datasets-used-in-this-tutorial">Datasets used in this Tutorial<a class="headerlink" href="#datasets-used-in-this-tutorial" title="Link zu dieser Sektion">¶</a></h3>
<p>In this tutorial we will make use of an airborne hyperspectral or imaging spectroscopy image that was collected with the HyMap sensor (<strong>HyMap_125bands_Karlsruhe2.tif</strong>). This dataset is comparable to a multispectral image but contains a lot more bands which continuously cover the spectra wavelengths regions between approximately 400 and 2400 nm. The HyMap image shows a forested area in the North of the German city Karlsruhe. The dataset has been described with more details in the following publications: </p>
<p>Fassnacht, F. E.; Neumann, C.; Forster, M.; Buddenbaum, H.; Ghosh, A.; Clasen, A.; Joshi, P. K.; Koch, B. (2014). Comparison of feature reduction algorithms for classifying tree species with hyperspectral data on three central european test sites. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7 (6), 2547–2561. doi:10.1109/JSTARS.2014.2329390</p>
<p>Ghosh, A.; Fassnacht, F. E.; Joshi, P. K.; Koch, B. (2014). A framework for mapping tree species combining Hyperspectral and LiDAR data: role of selected classifiers and sensor across three spatial scales. International journal of applied earth observation and geoinformation, 26, 49–63. doi:10.1016/j.jag.2013.05.017</p>
<p>Additionally, a point-Shapefile is provided which contains sample points (reference positions) for 5 tree species (50 points per species, 250 in total) (<strong>tree_species_KA.shp</strong>). These reference positions were collected using visual interpretation of high-resolution images in combination with reference tree species maps provided by the local forest administration. These reference tree species maps are also provided as tif-files (<strong>Reference_information.tif; Reference_information2.tif</strong>). The latter can used for comparison with the classification maps produced in the tutorial. </p>
<h3 id="step-1-loading-an-visualizing-the-hyperspectral-hymap-image-and-the-reference-dataset">Step 1: Loading an visualizing the hyperspectral HyMap image and the reference dataset<a class="headerlink" href="#step-1-loading-an-visualizing-the-hyperspectral-hymap-image-and-the-reference-dataset" title="Link zu dieser Sektion">¶</a></h3>
<p>As first step, load all necessary R packages by executing the following code:</p>
<div class="highlight"><pre><span></span><code>require(raster)
require(e1071)
require(caret)
require(rgdal)
require(matrixStats)
require(RStoolbox)
require(factoextra)
require(VSURF)
</code></pre></div>
<p>R will give you a warning message in case a package is not installed yet. If this is the case, please install the packages either through the main menu of Rstudio by selecting <strong>"Tools" =&gt;</strong> <strong>"Install packages"</strong> and then following the appearing dialogue, or by entering the corresponding R code to install the packages into the console. E.g., to install the package "raster" use the code:</p>
<div class="highlight"><pre><span></span><code>install.packages("raster")
</code></pre></div>
<p>After all packages are successfully installed, we will load the hyperspectral image and the shapefile containing the reference information. You should already be familiar with the corresponding code in R from earlier practicals of the course.</p>
<div class="highlight"><pre><span></span><code># change directory (depends on where you stored your data)
setwd("D:/Diplomado/Tut_superv_class_hyper/2_data")

# load hyperspectral image
hym_img &lt;- stack("HyMap_125bands_Karlsruhe2.tif")

# load reference data
ref &lt;- readOGR(".", "tree_species_KA")
</code></pre></div>
<p>After loading the hyperspectral image, we can obtain some basic information about the data using some standard commands of the Raster package:</p>
<div class="highlight"><pre><span></span><code># we can now obtain some information about the image using the following commands:

# number of colums of the hyperspectral image
ncol(hym_img)
# number of rows of the hyperspectral image
nrow(hym_img)
# number of bands of the hyperspectral image
length(hym_img@layers)
# summary information of all bands
hym_img@layers
# summary information of a specific band, here band number 5
hym_img@layers[5]
# check the geographic extent of the image
extent(hym_img)
# check the defined coordinate system
crs(hym_img)
</code></pre></div>
<p>While we do not exlicitly need these commands right now, it is still important to know them because you will regularly confront situations where you need them for processing your images in an effective way. Next, we will plot our hyperspectral image (<strong>hym_img</strong>) as well as the reference data stored in the loaded shapefile (<strong>ref</strong>).  You can see in the code below, that we will select the bands number 21, 14 and 7 to plot a RGB-composit of the hyperspectral image. This leads to an Color-Infrared-View of the image because the band 21 is located in the NIR, the band 14 in the red region and band 7 in the green region. We will furthermore plot the reference data on top of the image using the species information to color-code each sample. The species information is stored in the attribute table of the Shapefile in the column named "id" which can be accessed with the setting <strong>col=ref@data$id</strong>.</p>
<div class="highlight"><pre><span></span><code># plot CIR view of hyperspectral image
plotRGB(hym_img, r=21, g=14, b=7, stretch="hist")
# add reference points
par(new=T)
plot(ref, col=ref@data$id, add=T)
</code></pre></div>
<p>This will result in the following plot:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_1.png"><img alt="" src="../assets/Tut_hyp_1.png"/></a></p>
<p>In the given plot, coniferous stands are depicted in green and broadleaved stands in red. This is because broadleaved stands have a higher reflectance in the near-infrared region than coniferous stands. You can also see that the reference points are depicted in different colors and are mostly crowded in individual forest stands. This is due to the fact, that these stands contain mostly a single species and can hence serve as reliable reference areas to collect samples that we will use to train the algorithm. In total, we consider five tree species, that we will introduce later on. Already now, we can see that one species (Fagus sylvatica) is not occurring in pure stands (this can be seen because pure stands appear just in green or just in red colors). You can see that all the dark blue samples are occurring in rather heterogeneous stands with mixed green and red areas.</p>
<p>As next step, we will now extract the spectral values of the HyMap image at each of the 250 reference point locations. So we will have 50 spectral signatures for each tree species. To do this we run:</p>
<div class="highlight"><pre><span></span><code># extract spectral signatures at the reference data points
ref_data_tr &lt;- extract(hym_img, ref)
</code></pre></div>
<p>Depending on the performance of your computer, this will take a bit of time, as the image has quite a lot of bands. Once, we have extracted the values, we will transform the resulting data matrix into a data frame by running:</p>
<div class="highlight"><pre><span></span><code>ref_data_tr &lt;- as.data.frame(ref_data_tr)
</code></pre></div>
<p>Now, we can have a look at the extracted spectral signature by running the plot command in a loop to plot all spectral signatures into the same plot window:</p>
<div class="highlight"><pre><span></span><code># plot all spectra and color-code with species
dev.off()
for (i in 1:nrow(ref_data_tr)){

  par(new=T) 
  plot(1:125, ref_data_tr[i,], type="l", col=ref@data$id[i], ylim=c(0,6000), ylab="Reflectance", xlab="HyMap band")

}
</code></pre></div>
<p>This will result in the following image:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_2.png"><img alt="" src="../assets/Tut_hyp_2.png"/></a></p>
<p>What is not optimal in this graph is that the x-axis does not show the wavelengths, but the band number of the HyMap sensor. This can be fixed by providing the center-wavelengths of HyMap instead as an array instead of the simple sequence from 1 to 125 (the code part is: <strong>1:125</strong>) as done at the moment. However, because it is not really relevant for this tutorial, we will not spend more time on this for now.</p>
<p>In the plot, we used the species information to color-code the spectra, but as you can see this graph is quite chaotic, and it is hard to see where the spectra of one species ends and the spectra of another species start. All in all, there seems to be quite a heavy overlap between the spectral signatures of the five species. So let us try to make this plot a bit clearer by plotting the mean spectral signature of each species plus its standard deviations.</p>
<p>We will need a bit more code to do this. There are definitely ways to code this with less lines of code, however, the solution given below makes the logical flow quite clear and may be easier to understand for people who are not yet professionals in R. For detailed information, please read the comments in the code.</p>
<div class="highlight"><pre><span></span><code>##################################
# plot mean + sd of each species
##################################

# The species information stored in the Shapefile is coded with numbers. Below you can see which species
# belongs to which number code.

# species:
# id = 1 = Quercus robur/petreae (Common/Trembling Oak)
# id = 2 = Pinus sylvestris (Pine) 
# id = 3 = Quercus rubra (Red Oak)
# id = 4 = Fagus sylvatica (European beech)
# id = 5 = Pseudotsuga menziesii (Douglas fir)

# in the first step, this information about the species will be attached to the dataframe
# containing the spectral signatures at the reference plot locations:

ref_data_tr$id &lt;- ref@data$id

# now we are ready to calculate the mean and standard deviation (sd) spectral signatures of each species 
# to save the mean and sd signatures, we create an empty list object    
meansd &lt;- list()

# then we run a loop in which we first build a subset of the spectral signatures dataframe so that the
# subset only contains the spectra of a single species.
for (i2 in 1:5){

  # build subset of the dataframe where the species id = i2 
  sp &lt;- ref_data_tr[ref_data_tr$id == i2,]
  # from the remaining 50 spectral signatures of a single species, calculate the column mean and sd
  means &lt;- colMeans2(as.matrix(sp[,1:125]))
  sds &lt;- colSds(as.matrix(sp[,1:125]))
  # bind the two arrays containing the mean and sd values
  fin &lt;- cbind(means, sds)
  # save them to the list
  meansd[[i2]] &lt;- fin
  # repeat with next species

}

# now we are able to plot the mean and sd values of the five species

# close any currently open plot window
dev.off()
# plot an "empty" plot by setting the color to white. This is a trick to avoid overlapping axes-titles in the for-loop below
plot(1:125, meansd[[1]][,1], type="l", col="white", ylim=c(0,6000), ylab="Reflectance [10000=100%]", xlab="# Band")
# add a legend indicating the species names
legend("topright", legend=c("Quercus robur", "Pinus sylvestris", "Quercus rubra", "Fagus sylvatica", "Pseudotsuga menziesii"), col=c(1,2,3,4,5), lty=c(1,1,1,1,1))

# plot the mean spectral signature as well as two additional signatures which show the mean - sd and mean + sd to create an interval in which most of the spectral signatures of the species are located.

for (i3 in 1:5){


  par(new=T)
  # plot mean spectral signature
  plot(1:125, meansd[[i3]][,1], type="l", col=i3, ylim=c(0,6000), add=T, lwd=2, axes=F, ann=F)
  par(new=T)
  # plot mean spectral signature + sd values
  plot(1:125, meansd[[i3]][,1]+meansd[[i3]][,2], type="l", col=adjustcolor(i3, alpha.f=0.5), ylim=c(0,6000), add=T, lty=2, axes=F, ann=F)
  par(new=T)
  # plot mean spectral signature - sd values
  plot(1:125, meansd[[i3]][,1]-meansd[[i3]][,2], type="l", col=adjustcolor(i3, alpha.f=0.5), ylim=c(0,6000), add=T, lty=2, axes=F, ann=F)


}
</code></pre></div>
<p>This rather long code will lead to the following plot:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_3.png"><img alt="" src="../assets/Tut_hyp_3.png"/></a></p>
<p>This plot is a lot clearer. We can see the mean spectral signatures of all broadleaved tree species (Quercus robur, Quercus rubra and Fagus sylvatica) have a higher reflectance amplitude than the two coniferous species (Pinus sylvestris and Pseudotsuga Menziesii). We can also see that the SD-values of the spectral signatures of the individual species overlap quite notably. This suggests that the classification might not be that easy. But let us explore, what can be achieved.</p>
<h3 id="step-2-tree-species-classification-using-the-original-hymap-bands">Step 2: Tree species classification using the original HyMap bands<a class="headerlink" href="#step-2-tree-species-classification-using-the-original-hymap-bands" title="Link zu dieser Sektion">¶</a></h3>
<p>We will now prepare and run the first classification approach using all 125 bands of the HyMap image. For this, we first prepare two variables. One contains all the spectral signatures of the training data (<strong>trainval</strong>) and one that contains the corresponding response value (the information to which tree species the sample belongs - coded as a number between 1 and 5) (<strong>treespec</strong>).</p>
<div class="highlight"><pre><span></span><code>############################################
# prepare classification using original bands
############################################
trainval &lt;- ref_data_tr[,1:125]
treespec &lt;- ref_data_tr$id
</code></pre></div>
<p>As we can see, we obtain both of these variables from the <strong>ref_dara_tr</strong> dataframe which contains the spectral signature in columns 1 to 125 and the reference data in column 126 (the latter can also be directly accessed via the column name).</p>
<p>Now we are ready to start the classification. As you might already know, machine learning algorithms such as support vector machines, typically require a parameter tuning. That is, we try to identify the optimal settings of the classification algorithm by splitting the dataset into several parts and then use some of these parts to train the classifier (with a set of parameters) and then validate them with another part which has not been used during training. By repeatedly varying the parameters, we can find a combination of parameters, that performs exceptionally well.</p>
<p>In the case of SVM we can vary two parameters: gamma and cost. <strong>Gamma</strong> defines how flexible the so-called hyperplane is allowed to be. The hyperplane is basically the separation-"line" between the different classes in the feature space. On the other hand, <strong>cost</strong> describes how strongly the classifier punishes a wrongly classified sample. If cost and/or Gamma is too high, there is a risk of overfitting - however, by applying datasplits during the grid search, overfitted models will lead to bad performances. By using an automated grid-search which varies the gamma and cost values within a user-defined range, it is possible to automatically search for a good gamma/cost combination in R. The corresponding code looks like this (this might take a while to run, as a lot of different models are examined here): </p>
<div class="highlight"><pre><span></span><code># set a seed to allow for reproducing the results
set.seed(1173)
# set a range of gamma and cost values to be tested in the parameter tuning
gammat = seq(.1, .9, by = .1)
costt = seq(1,128, by = 12)
# check the parameters to be tested
gammat
costt   
# run the parameter tuning
tune1 &lt;- tune.svm(trainval, as.factor(treespec), gamma = gammat, cost=costt)
# plot the results of the parameter tuning
plot(tune1)
</code></pre></div>
<p>As you can see we will test 9 different values for gamma and 11 different cost-values. In theory, it would of course be possible to test even more combinations but the ones selected here are working quite well from our experience. However, feel free to try out some higher and lower values if you are interested in the results. The code above will lead to the following plot:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_4.png"><img alt="" src="../assets/Tut_hyp_4.png"/></a></p>
<p>This plot summarizes the performances obtained with the examined gamma and cost values. The legend is showing the error rate. So that means, the higher we set gamma, the larger the errors were during the parameter tuning. On the other hand, cost did not have a notable effect on the parameter tuning in the given example.</p>
<p>We will now extract the best identified gamma and cost value by running:</p>
<div class="highlight"><pre><span></span><code>gamma &lt;- tune1$best.parameters$gamma
cost &lt;- tune1$best.parameters$cost
</code></pre></div>
<p>Then we are ready to train the SVM classification. We will train two different models, one model in which all available data will be used for training the model (maximizing the available information) and a second model which is additionally applying a 10-fold cross-validation which we can use to get an idea of how well the classification is able to classify the reference samples correctly.</p>
<div class="highlight"><pre><span></span><code># train the model with all available samples
model &lt;- svm(trainval, as.factor(treespec), gamma = gamma, cost = cost, probability = TRUE)
# train model with 5-fold cross-validation to get first impression on accuracies
model2 &lt;- svm(trainval, as.factor(treespec), gamma = gamma, cost = cost, probability = TRUE, cross=10)
</code></pre></div>
<p>This should run quite fast and we can not have a look at the model results by running:</p>
<div class="highlight"><pre><span></span><code>summary(model2)
</code></pre></div>
<p>This should show something like this:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_5.png"><img alt="" src="../assets/Tut_hyp_5.png"/></a></p>
<p>So we can see that we have a mean overall accuracy of approximately 72%. This is not bad but also not really good. We can now also apply this classification model to the full image by running:</p>
<div class="highlight"><pre><span></span><code># set output directory to save the classification map   
setwd("D:/1_tree_species_Karlsruhe/2_data/results")
# apply the classification model to the hyperspectral image using the predict function
svmPred &lt;- predict(hym_img, model, filename="tree_spec_map.tif", na.rm=TRUE, progress='text', format='GTiff', datatype='INT1U',overwrite=TRUE)
</code></pre></div>
<p>This process may take a few minutes depending on the speed of your computer. We can then plot the predicted map using:</p>
<div class="highlight"><pre><span></span><code># plot the resulting tree species map
plot(svmPred)
legend("topright", legend=c("1 = Quercus robur", "2 = Pinus sylvestris", "3 = Quercus rubra", "4 = Fagus sylvatica", "5 = Pseudotsuga menziesii"))
</code></pre></div>
<p>This will result in the following tree species map:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_6.png"><img alt="" src="../assets/Tut_hyp_6.png"/></a></p>
<p>On the first glance, this map looks quite plausible. We can see that many of the stands from which we collected the training data (and we hence know, that they are composed of a single species) appear quite homogeneous in the classification map. The quality of the map could be examined with more details by loading the map into QGIS and compare it to the reference maps provided with the tutorial materials and maybe also by comparing the map with additional high-resolution Google or Bing maps which can be visualized in QGIS as well. </p>
<h3 id="step-3-tree-species-classification-using-pca-bands">Step 3: Tree species classification using PCA bands<a class="headerlink" href="#step-3-tree-species-classification-using-pca-bands" title="Link zu dieser Sektion">¶</a></h3>
<p>One way to improve the accuracy of supervised classifications, particularly if working with hyperspectral data are feature extraction methods as they allow to compress the feature space and hence reduce colinearity of predictors. In the following we will apply the well-known principal component analysis (PCA) to the hyperspectral image to reduce the original 125 bands to a smaller number of bands carrying most of the variability contained in the image.</p>
<p>To apply a PCA to a raster dataset, we can use the <strong>rasterPCA</strong> function of the RStoolbox package. The function actually uses only a subset of pixels to calculate the PCA transformation and then only applies the transformation to the full raster stack. In the example below, we use 1000 sample points. The more points are used, the longer the PCA calculation may last. We also set the spca parameter to TRUE to scale all the bands (this is not absolutely necessary but may be reasonable to do in our case as for example vegetation spectra have differing reflectance value ranges in the visual and near-infrared region; by scaling all value ranges of the bands, such constant differences are smoothed out). If you are interested whether this has any effect, you can also re-run the code later and switch off the spca parameter. To run the PCA in R we execute the following code (this may take a while!):</p>
<div class="highlight"><pre><span></span><code>############################################
# prepare classification using PCA
############################################

pca &lt;- rasterPCA(hym_img, nSamples=5000, spca=T)
</code></pre></div>
<p>From the resulting <strong>pca</strong> object, we can extract the transformation model:</p>
<div class="highlight"><pre><span></span><code>pca_ras &lt;- pca$map
</code></pre></div>
<p>As well as the new raster stack containing the PCA components (125 components/raster layers sorted from high to low information content)</p>
<div class="highlight"><pre><span></span><code>pca_mod &lt;- pca$model
</code></pre></div>
<p>To check how much of the overall variability the individual components are carrying, we can have a look at an eigenvalue scree plot using the <strong>fviz-eig</strong> function from the factoextra package:</p>
<div class="highlight"><pre><span></span><code>fviz_eig(pca_mod)
</code></pre></div>
<p>This will result in the following plot:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_7.png"><img alt="" src="../assets/Tut_hyp_7.png"/></a></p>
<p>As we can see, the first seven components seem to already carry the vast majority of the variability in the hyperspectral image. Starting from the 8<sup>th</sup> component, the information content seems to be rather low. However, we can also check this by plotting the individual components. We can for example compare the first three components with the last three components by running:</p>
<div class="highlight"><pre><span></span><code>x11()
dev.off()
par(mfrow=c(2,3))
plot(pca_ras[[c(1:3, 123:125)]])
</code></pre></div>
<p>this will result in the following plot:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_8.png"><img alt="" src="../assets/Tut_hyp_8.png"/></a></p>
<p>As we can see, the first three PCs show quite a lot of variation related to the image contents. PC3 does not show too many patterns, due to a few outlayer value in the urban area (the green dot) - but PC3 actually still does contain a lot variation (this would become visible if the image stretched would be adapted). On the other hand, the last three PCs seem to only show random noise. Based on the combination of the eigenvalue plot above and this visual examination we can now decide how many of the PCs we want to use in our classification. For now, we will simply select the first 15 components assuming that most of the variability in the image is covered by these components. Hence, we extract the values of the first 15 PCs from the reference sample locations using:</p>
<div class="highlight"><pre><span></span><code>trainval_pca &lt;- extract(pca_ras[[1:15]], ref)
</code></pre></div>
<p>Then, we run the same SVM classification as we did before, including also the parameter tuning:</p>
<div class="highlight"><pre><span></span><code>set.seed(1173)
# parameter tuning
gammat = seq(.1, .9, by = .1)
costt = seq(1,128, by = 12)
tune2 &lt;- tune.svm(trainval_pca, as.factor(treespec), gamma = gammat, cost=costt)
plot(tune2)
# extract best parameters
gamma2 &lt;- tune2$best.parameters$gamma
cost2 &lt;- tune2$best.parameters$cost

# train the model with all available samples
model_pca &lt;- svm(trainval_pca, as.factor(treespec), gamma = gamma2, cost = cost2, probability = TRUE)
# train model with 10-fold cross-validation to get first impression on accuracies
model_pca2 &lt;- svm(trainval_pca, as.factor(treespec), gamma = gamma2, cost = cost2, probability = TRUE, cross=10)
summary(model_pca2)
</code></pre></div>
<p>This will produce the following output (you will additionally see the plot from the parameter tuning, but you are already familiar with this step, so I will not repeat it here):</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_9.png"><img alt="" src="../assets/Tut_hyp_9.png"/></a></p>
<p>As we can see, the classification accuracy using only the first 15 PCA components as input is slightly better than the accuracy we obtained using all hyperspectral bands. Your results might vary slightly, depending on how you set your seed value in the function set.seed().</p>
<p>Finally, let us also produce a prediction map for this classification approach by running the following code:</p>
<div class="highlight"><pre><span></span><code># set output directory
setwd("D:/1_tree_species_Karlsruhe/2_data/results")
# apply classifier to full image    
svmPred_pca &lt;- predict(pca_ras[[1:15]], model_pca, filename="tree_spec_map_pca.tif", na.rm=TRUE, progress='text', format='GTiff', datatype='INT1U',overwrite=TRUE)
</code></pre></div>
<p>We can also have a look at this map by running:</p>
<div class="highlight"><pre><span></span><code>dev.off()
plot(svmPred_pca)
legend("topright", legend=c("1 = Quercus robur", "2 = Pinus sylvestris", "3 = Quercus rubra", "4 = Fagus sylvatica", "5 = Pseudotsuga menz."))
</code></pre></div>
<p>Which will result in:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_10.png"><img alt="" src="../assets/Tut_hyp_10.png"/></a></p>
<p>This map looks quite similar as the one from the preceeding classification. This is no surprise as the classification accuracies were also similar.</p>
<h3 id="step-4-tree-species-classification-using-a-combination-of-feature-selection-vsurf-and-feature-extraction-pca">Step 4: Tree species classification using a combination of feature selection (VSURF) and feature extraction (PCA)<a class="headerlink" href="#step-4-tree-species-classification-using-a-combination-of-feature-selection-vsurf-and-feature-extraction-pca" title="Link zu dieser Sektion">¶</a></h3>
<p>In this final step of the Tutorial, we will now add another step. We will use the first 30 components of the PCA as our starting input dataset. Then we will run a feature selection algorithm on these 30 components and use the selected components to train the SVM classification.</p>
<p>First we select the first 30 components of the PCA raster stack and extract the corresponding values:</p>
<div class="highlight"><pre><span></span><code>set.seed(23)
pca_ras2 &lt;- pca_ras[[1:30]]
trainval_pca2 &lt;- extract(pca_ras2, ref)
</code></pre></div>
<p>Then we run the feature selection algorithm VSURF. This algorithm is based on Random Forest and in some of our recent studies it performed quite well. Some more detailed descriptions on VSURF can be found in Genuer, R., Poggi, J.-M., Tuleau-Malot, C., 2015. VSURF: an r package for VariableSelection using random forests. R J. R Found. Statist. Comput. 7 (2), 19–33. Be aware that I set the option <strong>parallel</strong> to TRUE and <strong>ncores</strong> = 3. This means in this case, R will use 3 CPUs/cores to run the algorithm. In case your computer only has one core/CPU, you have to set parallel to FALSE. If you do not know how many cores your computer has, you can also try to run the code and if it fails, you can reduce the number of cores or deactivate the <strong>parallel</strong> setting.</p>
<div class="highlight"><pre><span></span><code>vsurf_pca &lt;- VSURF(trainval_pca2, as.factor(treespec), ntree=500, parallel = T, ncores = 3)
</code></pre></div>
<p>The resulting vsurf_pca object contains three sets of selected variables. In the following we will focus on the predictor set optimized for prediction (see Tutorial 2 of this module for more information).
The column ids of this set of selected variables can be accessed with the command:</p>
<div class="highlight"><pre><span></span><code>vsurf_pca$varselect.pred
</code></pre></div>
<p>If we run it, we can see the following output:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_11.png"><img alt="" src="../assets/Tut_hyp_11.png"/></a></p>
<p>We can see that the first three PCs got selected but also some lower ranked PCs like 26 and 27.</p>
<p>As next step, we will build a subset of our predictor feature space currently containing 30 PCs to a new dataframe containing only the 11 PCs selected by VSURF. For this we run:</p>
<div class="highlight"><pre><span></span><code>trainval_vsurf &lt;- trainval_pca2[,vsurf_pca$varselect.pred]
</code></pre></div>
<p>These predictors are then used in the SVM classification workflow as we already know it from STEP 2 and 3:</p>
<div class="highlight"><pre><span></span><code># run parameter tuning
set.seed(1173)
gammat = seq(.1, .9, by = .1)
costt = seq(1,128, by = 12)
tune3 &lt;- tune.svm(trainval_vsurf, as.factor(treespec), gamma = gammat, cost=costt)
plot(tune3)

# take best parameters
gamma3 &lt;- tune3$best.parameters$gamma
cost3 &lt;- tune3$best.parameters$cost


# train the model with all available samples
model_vsurf &lt;- svm(trainval_vsurf, as.factor(treespec), gamma = gamma3, cost = cost3, probability = TRUE)
# train model with 5-fold cross-validation to get first impression on accuracies
model_vsurf2 &lt;- svm(trainval_vsurf, as.factor(treespec), gamma = gamma3, cost = cost3, probability = TRUE, cross=10)
summary(model_vsurf2)
</code></pre></div>
<p>This will result in the following output:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_12.png"><img alt="" src="../assets/Tut_hyp_12.png"/></a></p>
<p>As we can see, we could increase the classification accuracy by almost 10% with this additional feature selection step. This is quite a lot! As last step, we can now also produce the corresponding classification map by running:</p>
<div class="highlight"><pre><span></span><code># set output directory to save classification map
setwd("D:/1_tree_species_Karlsruhe/2_data/results")
# apply classifier to full image
svmPred_vsurf &lt;- predict(pca_ras2[[vsurf_pca$varselect.pred]], model_vsurf, filename="tree_spec_map_vsurf.tif", na.rm=TRUE, progress='text', format='GTiff', datatype='INT1U',overwrite=TRUE)
</code></pre></div>
<p>And plot the map by running:</p>
<div class="highlight"><pre><span></span><code>dev.off()
plot(svmPred_vsurf)
legend("topright", legend=c("1 = Quercus robur", "2 = Pinus sylvestris", "3 = Quercus rubra", "4 = Fagus sylvatica", "5 = Pseudotsuga menz."))
</code></pre></div>
<p>Which will result in the following map:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-width="100%" href="../assets/Tut_hyp_13.png"><img alt="" src="../assets/Tut_hyp_13.png"/></a></p>
<p>We have now created three tree species maps using our hyperspectral image and the reference data. Feel free to have a closer look at the maps and think about how they differ and which one is agreeing best with the reference maps as well as with the visual impression when comparing the maps with high-resolution Google and Bing images in QGIS.</p>
<h3 id="exercise">Exercise<a class="headerlink" href="#exercise" title="Link zu dieser Sektion">¶</a></h3>
<p>If you want to practice a bit more with the code, try to run another classification applying the VSURF variable selection to the original 125 bands of HyMap. How does this affect the classification accuracy?</p>
</article>
</div>
</div>
<a class="md-top md-icon" data-md-component="top" hidden="" href="#">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
            Zurück zum Seitenanfang
          </a>
</main>
<footer class="md-footer">
<nav aria-label="Fußzeile" class="md-footer__inner md-grid">
<a aria-label="Zurück: lidR Lasercanning" class="md-footer__link md-footer__link--prev" href="../../1_Laserscanning_lidR/1_Laserscanning_lidR/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                  Zurück
                </span>
                lidR Lasercanning
              </div>
</div>
</a>
<a aria-label="Weiter: Analysing time series with BFAST and NPPHEN" class="md-footer__link md-footer__link--next" href="../../BFAST/BFAST/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                  Weiter
                </span>
                Analysing time series with BFAST and NPPHEN
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      Copyright © CC-BY-SA-4.0 license - FU-Berlin Fernerkundung und Geoinformatik
    </div>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["toc.integrate", "search.suggest", "search.highlight", "content.action.edit", "content.code.copy", "navigation.tracking", "navigation.tabs", "navigation.path", "navigation.top", "navigation.footer"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "In Zwischenablage kopiert", "clipboard.copy": "In Zwischenablage kopieren", "search.result.more.one": "1 weiteres Suchergebnis auf dieser Seite", "search.result.more.other": "# weitere Suchergebnisse auf dieser Seite", "search.result.none": "Keine Suchergebnisse", "search.result.one": "1 Suchergebnis", "search.result.other": "# Suchergebnisse", "search.result.placeholder": "Suchbegriff eingeben", "search.result.term.missing": "Es fehlt", "select.version": "Version ausw\u00e4hlen"}}</script>
<script src="../../assets/javascripts/bundle.2a6f1dda.min.js"></script>
<script src="../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>